{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYyPcd9LTgrn",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZYtZjU0NSPnn",
    "outputId": "7e20a971-dc76-4ea2-934d-0f26da0a11fb"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "adHsAHM25nUH"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir('/content/drive/MyDrive/Projects/Distillation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vK6w4whUST4C"
   },
   "outputs": [],
   "source": [
    "# !pip install datasets --quiet\n",
    "# !pip install evaluate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lVbqbSinPAFL",
    "outputId": "ba411837-b5d0-4761-c5d5-11f78fa5bf9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "k0nkgDqc_Gec"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Ensure experiment reproducibility\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBqMpR7VJg8g",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHLaVK4eQYIx"
   },
   "source": [
    "\n",
    "\n",
    "[HateXplain](https://huggingface.co/datasets/Hate-speech-CNERG/hatexplain) is a benchmark dataset for hate speech detection.   \n",
    "Each record in the dataset includes four fields:\n",
    "- id: A unique identifier for the post.\n",
    "- annotators: A list of annotations made by 3 different annotators. Each annotator entry includes:  \n",
    "  - label: The label given to the post. 0: hatespeech, 1: normal, 2: offensive.\n",
    "  - annotator_id: A unique identifier for the annotator.\n",
    "  - target: The target of the post.\n",
    "- rationales: A list of binary arrays. 1 indicates a token that can justify the label assigned by the annotator.\n",
    "\n",
    "- post_tokens: A list of tokens (words) of the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tj5inwykSApG",
    "outputId": "85a44eb2-0258-42b3-d515-3a84c34c2b46"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Hate-speech-CNERG/hatexplain\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I8RTITPFTpmL",
    "outputId": "2926d2c6-6e5e-48ee-b3fa-7c9447c8e543"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'annotators', 'rationales', 'post_tokens'],\n",
      "        num_rows: 15383\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'annotators', 'rationales', 'post_tokens'],\n",
      "        num_rows: 1922\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'annotators', 'rationales', 'post_tokens'],\n",
      "        num_rows: 1924\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SyUS4Nk3TsXO",
    "outputId": "1118f0a5-1bab-47f4-ffba-1500144bdddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '23107796_gab', 'annotators': {'label': [0, 2, 2], 'annotator_id': [203, 204, 233], 'target': [['Hindu', 'Islam'], ['Hindu', 'Islam'], ['Hindu', 'Islam', 'Other']]}, 'rationales': [[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'post_tokens': ['u', 'really', 'think', 'i', 'would', 'not', 'have', 'been', 'raped', 'by', 'feral', 'hindu', 'or', 'muslim', 'back', 'in', 'india', 'or', 'bangladesh', 'and', 'a', 'neo', 'nazi', 'would', 'rape', 'me', 'as', 'well', 'just', 'to', 'see', 'me', 'cry']}\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Tokenizer and Teacher Model (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# tokenizer\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# teacher model\n",
    "ID2LABEL = {0: \"hate_speech\", 1: \"normal\", 2: \"offensive\"}\n",
    "LABEL2ID = {label: idx for idx, label in ID2LABEL.items()}\n",
    "NUM_LABELS = len(ID2LABEL)\n",
    "\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels = NUM_LABELS,\n",
    "    id2label = ID2LABEL,\n",
    "    label2id = LABEL2ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZQ0pniIrOxV",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjcRFH-ef0KG"
   },
   "source": [
    "- Use the tokenizer from the pretrained BERT model (bert-base-uncased) to tokenize input text.\n",
    "\n",
    "- Use majority vote to select the most common label as the final label.\n",
    "\n",
    "- Create PyTorch DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CanlWeoF8co5"
   },
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# from transformers import DataCollatorWithPadding\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# \"\"\"\n",
    "# Use DataCollatorWithPadding. Dynamic padding, save memory\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# def preprocess_function(example):\n",
    "#     \"\"\"\n",
    "#     Preprocesses a dataset example by tokenizing text and applying majority vote on annotator labels.\n",
    "#     \"\"\"\n",
    "#     texts = \" \".join(example[\"post_tokens\"])\n",
    "#     output = tokenizer(texts, max_length=128, truncation=True)  # padding is False\n",
    "\n",
    "#     labels = example[\"annotators\"][\"label\"]\n",
    "#     output[\"label\"] = Counter(labels).most_common(1)[0][0]\n",
    "\n",
    "#     return output\n",
    "\n",
    "\n",
    "# def create_dataloaders(dataset, batch_size):\n",
    "#     data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "#     train_dataset = dataset[\"train\"].shuffle(seed=42)\n",
    "#     eval_dataset = dataset[\"validation\"].shuffle(seed=42)\n",
    "#     test_dataset = dataset[\"test\"].shuffle(seed=42)\n",
    "\n",
    "#     train_loader = DataLoader(\n",
    "#         train_dataset,\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=True,\n",
    "#         collate_fn=data_collator\n",
    "#     )\n",
    "\n",
    "#     eval_loader = DataLoader(\n",
    "#         eval_dataset,\n",
    "#         batch_size=batch_size,\n",
    "#         collate_fn=data_collator\n",
    "#     )\n",
    "\n",
    "#     test_loader = DataLoader(\n",
    "#         test_dataset,\n",
    "#         batch_size=batch_size,\n",
    "#         collate_fn=data_collator\n",
    "#     )\n",
    "#     return train_loader, eval_loader, test_loader\n",
    "\n",
    "\n",
    "# dataset_preprocessed = dataset.map(preprocess_function)  # batched is False\n",
    "# dataset_preprocessed = dataset_preprocessed.remove_columns([\"id\", \"annotators\", \"rationales\", \"post_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\"\"\"\n",
    "Use DataCollatorWithPadding. Dynamic padding, save memory\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Preprocesses a batch of examples. Tokenize text. Apply majority vote on annotator labels.\n",
    "    \"\"\"\n",
    "    texts = [\" \".join(tokens) for tokens in examples[\"post_tokens\"]]\n",
    "    output = tokenizer(texts, max_length=128, truncation=True)  # padding is False\n",
    "    \n",
    "    output[\"label\"] = [Counter(annotator[\"label\"]).most_common(1)[0][0] for annotator in examples[\"annotators\"]]\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_dataloaders(dataset, batch_size):\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    train_dataset = dataset[\"train\"].shuffle(seed=42)\n",
    "    eval_dataset = dataset[\"validation\"].shuffle(seed=42)\n",
    "    test_dataset = dataset[\"test\"].shuffle(seed=42)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    eval_loader = DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    return train_loader, eval_loader, test_loader\n",
    "\n",
    "\n",
    "dataset_preprocessed = dataset.map(preprocess_function, batched=True)\n",
    "dataset_preprocessed = dataset_preprocessed.remove_columns([\"id\", \"annotators\", \"rationales\", \"post_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_eY-JDdMzcU",
    "outputId": "52a97f9a-33c5-472a-fada-1252c14b30ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
      "        num_rows: 15383\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
      "        num_rows: 1922\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
      "        num_rows: 1924\n",
      "    })\n",
      "})\n",
      "-- Example --\n",
      "{'input_ids': [101, 1057, 2428, 2228, 1045, 2052, 2025, 2031, 2042, 15504, 2011, 18993, 7560, 2030, 5152, 2067, 1999, 2634, 2030, 7269, 1998, 1037, 9253, 6394, 2052, 9040, 2033, 2004, 2092, 2074, 2000, 2156, 2033, 5390, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 2}\n"
     ]
    }
   ],
   "source": [
    "# Example dataset_preprocessed\n",
    "print(dataset_preprocessed)\n",
    "print(\"-- Example --\")\n",
    "print(dataset_preprocessed['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "srzy1BOZ6nMV"
   },
   "outputs": [],
   "source": [
    "# # Example train_loader\n",
    "# train_loader, eval_loader, test_loader = create_dataloaders(dataset_preprocessed, batch_size=32)\n",
    "\n",
    "# for batch in train_loader:\n",
    "#     for k, v in batch.items():\n",
    "#         print(f\"{k}: {v.shape}\")   \n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEPXGEmoqokS",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uOg8SlLf4pa"
   },
   "source": [
    "\n",
    "> The student has the same general architecture as BERT.\n",
    "- The token-type embeddings and the pooler are removed while the number of layers is reduced by a factor of 2.\n",
    "- Most of the operations used in the Transformer architecture—linear layer and layer normalisation—are highly optimized in modern linear algebra frameworks.\n",
    "- Our investigations showed that variations on the last dimension of the tensor (hidden size dimension) have a smaller impact on computation efficiency (for a fixed parameters budget) than variations on other factors like the number of layers.\n",
    "- Thus we focus on reducing the number of layers.\n",
    ">\n",
    "> — Sanh, Victor, et al. *DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.* arXiv:1910.01108 (2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GQZ0zV5B7r1"
   },
   "source": [
    "## Teacher Model (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "h26-h3BVoij-"
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSequenceClassification\n",
    "# help(AutoModelForSequenceClassification.from_pretrained)\n",
    "\n",
    "# from transformers import PretrainedConfig\n",
    "# help(PretrainedConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ALRYJnPORrw",
    "outputId": "9b114ec8-0ad6-46eb-a0df-45c18605fd92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"hate_speech\",\n",
       "    \"1\": \"normal\",\n",
       "    \"2\": \"offensive\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"hate_speech\": 0,\n",
       "    \"normal\": 1,\n",
       "    \"offensive\": 2\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.51.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0OSTDbHOZo_"
   },
   "source": [
    "## Student Model (DistilBERT)\n",
    "\n",
    "- The student model has the same configuration as the teacher model, but with half the number of layers.\n",
    "- Initialize the layers of student model. Copy one out of two encoder layers, and all other layers from the teacher model.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Slyr60okCKq-"
   },
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import BertConfig\n",
    "\n",
    "\n",
    "def build_student_model(teacher_model):\n",
    "    student_config = teacher_model.config.to_dict()\n",
    "    student_config['num_hidden_layers'] //= 2  # half the number of hidden layer\n",
    "    student_config = BertConfig.from_dict(student_config)\n",
    "    model = type(teacher_model)(student_config)\n",
    "    return model\n",
    "\n",
    "\n",
    "def init_student_layers(teacher, student, use_layer = \"odd\"):\n",
    "    \"\"\"\n",
    "    Initialize the layers of student model.\n",
    "    Copy one out of two encoder layers, and all other layers from the teacher model.\n",
    "\n",
    "    Params:\n",
    "        teacher: The teacher model.\n",
    "        student: The student model.\n",
    "        use_layer: Whether to use odd {1, 3, 5, 7, 9, 11} or even {2, 4, 6, 8, 10, 12} layers from the teacher model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy one out of two encoder layers\n",
    "    teacher_encoder_layers = teacher.bert.encoder.layer\n",
    "    student_encoder_layers = student.bert.encoder.layer\n",
    "    # print(len(teacher_encoder_layers), len(student_encoder_layers))  # 12, 6\n",
    "    layer_indices = range(1, 12, 2) if use_layer == \"odd\" else range(0, 12, 2)  #  select odd or even layers\n",
    "    for i, layer_idx in enumerate(layer_indices):\n",
    "        student_encoder_layers[i].load_state_dict(teacher_encoder_layers[layer_idx].state_dict())\n",
    "\n",
    "    # Copy all other layers\n",
    "    student.bert.embeddings.load_state_dict(teacher.bert.embeddings.state_dict())\n",
    "    student.bert.pooler.load_state_dict(teacher.bert.pooler.state_dict())\n",
    "    student.classifier.load_state_dict(teacher.classifier.state_dict())\n",
    "\n",
    "    return student\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
    "\n",
    "\n",
    "# student_model = build_student_model(teacher_model)\n",
    "# print(student_model)\n",
    "# student_model = init_student_layers(teacher_model, student_model, use_layer=\"odd\")\n",
    "# print()\n",
    "# print('Teacher parameters :', count_parameters(teacher_model))\n",
    "# print('Student parameters :', count_parameters(student_model))\n",
    "# print('Compression ratio:', count_parameters(student_model) / count_parameters(teacher_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3Q4oScygXSL",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smpHkpaqSYZu"
   },
   "source": [
    "## Weighted Loss\n",
    "Let $\\alpha$, $\\beta$ be weighting factors, $0 < \\alpha, \\beta < 1$ and $\\alpha + \\beta < 1$,\n",
    "$$\\mathcal{L} = \\alpha \\mathcal{L}_\\text{cls} + \\beta \\mathcal{L}_\\text{KD} + (1- \\alpha - \\beta)\\mathcal{L}_{\\text{cos}}$$\n",
    "\n",
    "## Softmax\n",
    "The softmax function converts a vector of raw scores (logits) $\\pmb{z} = [z^{(1)}, z^{(2)}, \\cdots, z^{(k)}]$ into a probability distribution over $k$ classes. The temperature parameter $T > 0$ controls the smoothness of the resulting probability distribution.\n",
    "$$\n",
    "\\text{softmax}(\\pmb{z}^{(j)}, T) = \\frac{ \\exp ( \\pmb{z}^{(j)}  / T) }{\\sum_{q=1}^{k} \\exp ( \\pmb{z}^{(q)} / T )}\n",
    "$$\n",
    "$T=1$, standard softmax;  \n",
    "$T < 1$, sharper distribution, more confident predictions (the largest logits dominate);  \n",
    "$T > 1$, smoother distribution, more uncertainty (probabilities are more uniform).\n",
    "\n",
    "## Cross-Entropy Loss\n",
    "Let $\\pmb{p}$ be the target probability distribution and $\\pmb{q}$ be the input probability distribution, both over $k$ classes. The Cross-Entropy measures the difference between these two distributions as:\n",
    "$$\n",
    "H(\\pmb{p},\\pmb{q})= - \\sum_{j=1}^{k} \\pmb{p}^{(j)} \\log \\pmb{q}^{(j)}\n",
    "$$\n",
    "\n",
    "## KL Divergence\n",
    "$$D_{\\mathrm{KL}}(\\pmb{p} \\,\\|\\, \\pmb{q}) = \\sum_{j=1}^{k} \\pmb{p}^{(j)} \\log \\frac{\\pmb{p}^{(j)}}{\\pmb{q}^{(j)}} = \\sum_{j=1}^{k} \\pmb{p}^{(j)} (\\log \\pmb{p}^{(j)} - \\log \\pmb{q}^{(j)})$$\n",
    "\n",
    "$$\n",
    "H(\\pmb{p}, \\pmb{q}) = H(\\pmb{p}) + D_{\\mathrm{KL}}(\\pmb{p} \\,\\|\\, \\pmb{q})\n",
    "$$\n",
    "Since $H(\\pmb{p})$ is fixed, minimizing cross-entropy is equivalent to minimizing the KL divergence between $\\pmb{p}$ and $\\pmb{q}$.\n",
    "\n",
    "## Classification Loss\n",
    "$$\\mathcal{L}_\\text{cls} = H( \\pmb{y}_\\text{true}, \\pmb{y}_\\text{pred\\_student} )$$\n",
    "\n",
    "## Knowledge Distillation Loss\n",
    "Both cross-entropy and KL divergence can be used for knowledge distillation loss, as they result in the same gradients and optimization updates.\n",
    "\n",
    "KL divergence is often preferred, since it equals to zero when the student's output distribution exactly matches the teacher's. While cross-entropy includes an constant offset — the entropy $H(\\pmb{p})$ of the teacher's output distribution. This constant does not affect the optimization process, but it introduces noise into the loss value across batches, making training curves less interpretable.\n",
    "\n",
    "The multiplication by $T^2$ corrects for the gradient scaling effect introduced by the temperature during backpropagation.\n",
    "\n",
    "$$\\mathcal{L}_\\text{KD} = H(\\text{softmax}(\\pmb{z}_{\\text{teacher}}, T), \\text{softmax}(\\pmb{z}_{\\text{student}}, T)) \\cdot T^2 $$\n",
    "\n",
    "$$\\mathcal{L}_\\text{KD} = D_{\\mathrm{KL}}(\\text{softmax}(\\pmb{z}_{\\text{teacher}}, T) \\,\\|\\, \\text{softmax}(\\pmb{z}_{\\text{student}}, T)) \\cdot T^2 $$\n",
    "\n",
    "In PyTorch, `F.cross_entropy(input, target)` expects logits as `input`, and class indices or class probabilities as `target`.   \n",
    "`F.kl_div(input, target)` expects log-probabilities as `input`, and probabilities as `target`.\n",
    "\n",
    "## Cosine Embedding Loss\n",
    "$$\\mathcal{L}_\\text{cos} = 1 - \\text{cosine\\_similarity}(\\pmb{z}_\\text{student}, \\pmb{z}_\\text{teacher})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "yH_NxZ8inOGy"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class KDLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute the knowledge distillation loss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, logits_student, logits_teacher, T):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            logits_student: The logits of the student model.\n",
    "            logits_teacher: The logits of the teacher model.\n",
    "            T: The temperature parameter.\n",
    "        \"\"\"\n",
    "        loss = nn.KLDivLoss(reduction='batchmean')(\n",
    "            F.log_softmax(logits_student/T, dim=-1),\n",
    "            F.softmax(logits_teacher/T, dim=-1)\n",
    "        ) * T * T\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "# # Loss functions\n",
    "# criterion_cls = nn.CrossEntropyLoss()  # Classification Loss\n",
    "# criterion_kd = KDLoss()  # Knowledge Distillation Loss\n",
    "# criterion_cos = nn.CosineEmbeddingLoss()  # Cosine Embedding Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJBIKCVWAVbP",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "kXhTxirJ1ZA0"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    accuracy = accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"]    \n",
    "    f1 = f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "2F-B5UuzEdda"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "\n",
    "class KDTrainer:\n",
    "    def __init__(self, teacher_model, student_model, train_loader, eval_loader, optimizer, scheduler=None, save_name=\"exp1\"):\n",
    "        self.teacher_model = teacher_model\n",
    "        self.student_model = student_model\n",
    "        self.train_loader = train_loader\n",
    "        self.eval_loader = eval_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "        self.criterion_cls = None\n",
    "        self.criterion_kd = None\n",
    "        self.criterion_cos = None\n",
    "        self._init_criterions()\n",
    "\n",
    "        self.save_name = save_name\n",
    "        self.writer = SummaryWriter(log_dir=f\"logs/{save_name}\")  # Tensorboard writer\n",
    "\n",
    "    def _init_criterions(self):\n",
    "        self.criterion_cls = nn.CrossEntropyLoss()\n",
    "        self.criterion_kd = KDLoss()\n",
    "        self.criterion_cos = nn.CosineEmbeddingLoss()\n",
    "\n",
    "    def train(self, num_epochs, T=1, alpha=0.5, beta=0.3):\n",
    "        train_histories, eval_histories = [], []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\n-- Epoch {epoch + 1}/{num_epochs} --\")\n",
    "\n",
    "            train_history = self.train_one_epoch(T, alpha, beta)\n",
    "            eval_history = self.evaluate()\n",
    "\n",
    "            train_histories.append(train_history)\n",
    "            eval_histories.append(eval_history)\n",
    "\n",
    "            # print results\n",
    "            print(f\"Train\\tLoss: {train_history['train_loss']:.4f}\")\n",
    "            print(f\"\\tcls: {train_history['train_loss_cls']:.4f}\")\n",
    "            print(f\"\\tkd: {train_history['train_loss_kd']:.4f}\")\n",
    "            print(f\"\\tcos: {train_history['train_loss_cos']:.4f}\")\n",
    "            print(f\"Eval\\tLoss: {eval_history['eval_loss']:.4f}\")\n",
    "            print(f\"\\tAccuracy: {eval_history['eval_accuracy']:.4f}\")\n",
    "            print(f\"\\tF1: {eval_history['eval_f1']:.4f}\")\n",
    "\n",
    "            # Log to TensorBoard\n",
    "            self.writer.add_scalar(\"Loss/train\", train_history['train_loss'], epoch)\n",
    "            self.writer.add_scalar(\"Loss/train_cls\", train_history['train_loss_cls'], epoch)\n",
    "            self.writer.add_scalar(\"Loss/train_kd\", train_history['train_loss_kd'], epoch)\n",
    "            self.writer.add_scalar(\"Loss/train_cos\", train_history['train_loss_cos'], epoch)\n",
    "            self.writer.add_scalar(\"Loss/eval\", eval_history['eval_loss'], epoch)\n",
    "            self.writer.add_scalar(\"Metrics/accuracy\", eval_history['eval_accuracy'], epoch)\n",
    "            self.writer.add_scalar(\"Metrics/f1\", eval_history['eval_f1'], epoch)\n",
    "\n",
    "        self.writer.close()  # close Tensorboard writer\n",
    "        \n",
    "        return train_histories, eval_histories\n",
    "\n",
    "    def train_one_epoch(self, T, alpha, beta):\n",
    "        self.teacher_model.eval()\n",
    "        self.student_model.train()\n",
    "\n",
    "        total = 0\n",
    "        total_loss, total_loss_cls, total_loss_kd, total_loss_cos = 0, 0, 0, 0\n",
    "\n",
    "        for batch in tqdm(self.train_loader, desc=\"Training\", leave=False):\n",
    "            # move batch data to device\n",
    "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "            # clear grad\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            outputs_student = self.student_model(**batch)\n",
    "            with torch.no_grad():\n",
    "                outputs_teacher = self.teacher_model(**batch)\n",
    "\n",
    "            # compute loss\n",
    "            loss_cls = self.criterion_cls(outputs_student.logits, batch[\"labels\"])\n",
    "            loss_kd = self.criterion_kd(outputs_student.logits, outputs_teacher.logits, T)\n",
    "            loss_cos = self.criterion_cos(\n",
    "                outputs_teacher.logits,\n",
    "                outputs_student.logits,\n",
    "                torch.ones(outputs_teacher.logits.size(0)).to(DEVICE)\n",
    "            )\n",
    "            loss = alpha * loss_cls + beta * loss_kd + (1.0 - alpha - beta) * loss_cos\n",
    "\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            if self.scheduler:\n",
    "                self.scheduler.step()\n",
    "\n",
    "            # total loss\n",
    "            batch_size = batch[\"labels\"].size(0)\n",
    "            total += batch_size\n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_loss_cls += loss_cls.item() * batch_size\n",
    "            total_loss_kd += loss_kd.item() * batch_size\n",
    "            total_loss_cos += loss_cos.item() * batch_size\n",
    "\n",
    "        # average loss\n",
    "        history = {\n",
    "            \"train_loss\": total_loss / total,\n",
    "            \"train_loss_cls\": total_loss_cls / total,\n",
    "            \"train_loss_kd\": total_loss_kd / total,\n",
    "            \"train_loss_cos\": total_loss_cos / total,\n",
    "        }\n",
    "\n",
    "        return history\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.student_model.eval()\n",
    "\n",
    "        total = 0\n",
    "        total_loss = 0\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.eval_loader, desc=\"Evaluating\", leave=False):\n",
    "                # move batch data to device\n",
    "                batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "                # forward\n",
    "                outputs = self.student_model(**batch)\n",
    "\n",
    "                # loss\n",
    "                loss = self.criterion_cls(outputs.logits, batch[\"labels\"])\n",
    "\n",
    "                # total loss\n",
    "                batch_size = batch[\"labels\"].size(0)\n",
    "                total += batch_size\n",
    "                total_loss += loss.item() * batch_size\n",
    "\n",
    "                preds = outputs.logits.argmax(dim=-1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "        # average loss\n",
    "        eval_loss = total_loss / total\n",
    "\n",
    "        accuracy, f1 = compute_metrics(all_preds, all_labels)\n",
    "\n",
    "        history = {\n",
    "            \"eval_loss\": eval_loss,\n",
    "            \"eval_accuracy\": accuracy,\n",
    "            \"eval_f1\": f1\n",
    "        }\n",
    "\n",
    "        return history\n",
    "\n",
    "    def save_model(self):\n",
    "        save_path = f\"models/{self.save_name}.pt\"\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        torch.save(self.student_model.state_dict(), save_path)\n",
    "        print(f\"Student model saved to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0Z9xVoU09Rq",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347,
     "referenced_widgets": [
      "8a85991934fc4e3daf64a1fd63274863",
      "eb1018d808b24f49a2f30caa847f5cba",
      "3fd8127f712b475c9a351e21c9e2ab4f",
      "145cfbf57a2f4fbca4b2a3e6dae1fd33",
      "7063860e7c684b27bb55c46ff1a56054",
      "72930a5ca24f443ba41f1df0135e0452",
      "9fea38a3101f41b28063500cee8b18f8",
      "9a36f9142b4f4a6cb6f42d569ef64b13",
      "3d1ad685658f49108eb108c12b16fb3f",
      "07b21e4340ec48c18b2d81eb5f495361",
      "00060cf7dea24f56b875bfea028ba90f",
      "c7e28425d0514a37b272e0a32285217f",
      "d9ad57fa7b294301ae35dac1821ea6b8",
      "8a490deaaaba4eada516ffd5ba1e6f92",
      "9af83f0a48344c84bfa756287c1135a4",
      "147523338f4c40499bfb56ccd31e63f5",
      "be0c5dcfed7d4baf8013bb55b26f486d",
      "9d0a76945caf48968d28c1219d32d6e7",
      "8840f3af4c2e40b29f8d7d89aa15cffc",
      "26e62d11a7bb420c86ca31482bbcf3ca",
      "fb1205383e0c4d4881faef0431777266",
      "40a5ac0a4ac6418b8c0e5bd40fcee5db"
     ]
    },
    "id": "PnIpLmhC-YAG",
    "outputId": "6431f2f2-b0c8-4996-9234-6c15b6a981b0"
   },
   "source": [
    "import torch.optim as optim\n",
    "from transformers import get_scheduler\n",
    "\n",
    "LEARNING_RATE = 5e-5\n",
    "NUM_EPOCHS = 5\n",
    "T = 1\n",
    "BATCH_SIZE = 32\n",
    "ALPHA = 1/3\n",
    "BETA = 1/3\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader, eval_loader, test_loader = create_dataloaders(dataset_preprocessed, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Init student model\n",
    "student_model = build_student_model(teacher_model)\n",
    "student_model = init_student_layers(teacher_model, student_model, use_layer=\"odd\")\n",
    "\n",
    "# To device\n",
    "teacher_model = teacher_model.to(DEVICE)\n",
    "student_model = student_model.to(DEVICE)\n",
    "\n",
    "print('Teacher parameters:', count_parameters(teacher_model))\n",
    "print('Student parameters:', count_parameters(student_model))\n",
    "compression_ratio = count_parameters(student_model) / count_parameters(teacher_model)\n",
    "print(f'Compression ratio: {compression_ratio:.4f}')\n",
    "\n",
    "optimizer = optim.AdamW(student_model.parameters(), lr=LEARNING_RATE)\n",
    "num_training_steps = NUM_EPOCHS * len(train_loader)\n",
    "scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "kdtrainer = KDTrainer(\n",
    "    teacher_model=teacher_model,\n",
    "    student_model=student_model,\n",
    "    train_loader=train_loader,\n",
    "    eval_loader=eval_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    save_name=\"exp1\"\n",
    ")\n",
    "\n",
    "_, _ = kdtrainer.train(num_epochs=NUM_EPOCHS, T=T, alpha=ALPHA, beta=BETA)\n",
    "\n",
    "kdtrainer.save_model()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir=logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA_BETAS = [\n",
    "    (0.5, 0.3),\n",
    "    (0.7, 0.2)\n",
    "]\n",
    "TEMPERATURES = [1, 2, 3]\n",
    "LEARNING_RATES = [5e-5, 3e-5, 1e-5]\n",
    "NUM_EPOCHS = [5]\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import torch.optim as optim\n",
    "from transformers import get_scheduler\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "\n",
    "configs = []\n",
    "\n",
    "for alpha_beta, T, lr, num_epochs in product(ALPHA_BETAS, TEMPERATURES, LEARNING_RATES, NUM_EPOCHS):\n",
    "    alpha, beta = alpha_beta\n",
    "    print(f\"\\nRunning config: alpha={alpha:.2f}, beta={beta:.2f}, T={T}, lr={lr}, num_epochs={num_epochs}\")\n",
    "    save_name = f\"A{alpha:.2f}_T{T}_LR{lr}_NE{num_epochs}\"\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader, eval_loader, test_loader = create_dataloaders(dataset_preprocessed, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Init student model\n",
    "    student_model = build_student_model(teacher_model)\n",
    "    student_model = init_student_layers(teacher_model, student_model, use_layer=\"odd\")\n",
    "    \n",
    "    # To device\n",
    "    teacher_model = teacher_model.to(DEVICE)\n",
    "    student_model = student_model.to(DEVICE)\n",
    "\n",
    "    optimizer = optim.AdamW(student_model.parameters(), lr=lr)\n",
    "    scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_epochs * len(train_loader)\n",
    "    )\n",
    "\n",
    "    kdtrainer = KDTrainer(\n",
    "        teacher_model=teacher_model,\n",
    "        student_model=student_model,\n",
    "        train_loader=train_loader,\n",
    "        eval_loader=eval_loader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        save_name=save_name\n",
    "    )\n",
    "\n",
    "    _, eval_metrics = kdtrainer.train(\n",
    "        num_epochs=num_epochs,\n",
    "        T=T,\n",
    "        alpha=alpha,\n",
    "        beta=beta\n",
    "    )\n",
    "    \n",
    "    kdtrainer.save_model()\n",
    "    \n",
    "    f1 = eval_metrics[-1][\"eval_f1\"]\n",
    "    configs.append({\n",
    "        \"alpha\": alpha,\n",
    "        \"beta\": beta,\n",
    "        \"T\": T,\n",
    "        \"lr\": lr,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"eval_f1\": f1\n",
    "    })\n",
    "    \n",
    "    df = pd.DataFrame(configs)\n",
    "    df.to_csv(\"grid_results.csv\", index=False)\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "qYyPcd9LTgrn",
    "m3Q4oScygXSL"
   ],
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (rag)",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00060cf7dea24f56b875bfea028ba90f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "07b21e4340ec48c18b2d81eb5f495361": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "145cfbf57a2f4fbca4b2a3e6dae1fd33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_07b21e4340ec48c18b2d81eb5f495361",
      "placeholder": "​",
      "style": "IPY_MODEL_00060cf7dea24f56b875bfea028ba90f",
      "value": " 481/481 [02:32&lt;00:00,  3.41it/s]"
     }
    },
    "147523338f4c40499bfb56ccd31e63f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "26e62d11a7bb420c86ca31482bbcf3ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3d1ad685658f49108eb108c12b16fb3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3fd8127f712b475c9a351e21c9e2ab4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9a36f9142b4f4a6cb6f42d569ef64b13",
      "max": 481,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3d1ad685658f49108eb108c12b16fb3f",
      "value": 481
     }
    },
    "40a5ac0a4ac6418b8c0e5bd40fcee5db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7063860e7c684b27bb55c46ff1a56054": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "72930a5ca24f443ba41f1df0135e0452": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8840f3af4c2e40b29f8d7d89aa15cffc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a490deaaaba4eada516ffd5ba1e6f92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8840f3af4c2e40b29f8d7d89aa15cffc",
      "max": 61,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_26e62d11a7bb420c86ca31482bbcf3ca",
      "value": 61
     }
    },
    "8a85991934fc4e3daf64a1fd63274863": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_eb1018d808b24f49a2f30caa847f5cba",
       "IPY_MODEL_3fd8127f712b475c9a351e21c9e2ab4f",
       "IPY_MODEL_145cfbf57a2f4fbca4b2a3e6dae1fd33"
      ],
      "layout": "IPY_MODEL_7063860e7c684b27bb55c46ff1a56054"
     }
    },
    "9a36f9142b4f4a6cb6f42d569ef64b13": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9af83f0a48344c84bfa756287c1135a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb1205383e0c4d4881faef0431777266",
      "placeholder": "​",
      "style": "IPY_MODEL_40a5ac0a4ac6418b8c0e5bd40fcee5db",
      "value": " 60/61 [00:03&lt;00:00, 15.70it/s]"
     }
    },
    "9d0a76945caf48968d28c1219d32d6e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9fea38a3101f41b28063500cee8b18f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "be0c5dcfed7d4baf8013bb55b26f486d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c7e28425d0514a37b272e0a32285217f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d9ad57fa7b294301ae35dac1821ea6b8",
       "IPY_MODEL_8a490deaaaba4eada516ffd5ba1e6f92",
       "IPY_MODEL_9af83f0a48344c84bfa756287c1135a4"
      ],
      "layout": "IPY_MODEL_147523338f4c40499bfb56ccd31e63f5"
     }
    },
    "d9ad57fa7b294301ae35dac1821ea6b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be0c5dcfed7d4baf8013bb55b26f486d",
      "placeholder": "​",
      "style": "IPY_MODEL_9d0a76945caf48968d28c1219d32d6e7",
      "value": "Evaluating:  98%"
     }
    },
    "eb1018d808b24f49a2f30caa847f5cba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_72930a5ca24f443ba41f1df0135e0452",
      "placeholder": "​",
      "style": "IPY_MODEL_9fea38a3101f41b28063500cee8b18f8",
      "value": "Training: 100%"
     }
    },
    "fb1205383e0c4d4881faef0431777266": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
